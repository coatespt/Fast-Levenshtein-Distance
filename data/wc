7d6
< %\documentclass[html]{book}    % Specifies the document style. 
11d9
< \author{Peter Coates}    % Declares the author's name. 
15d12
< \def\hpicwidth{1.33in}
18d14
< \begin{document}           % End of preamble and beginning of text. 
21d16
< \maketitle                 % Produces the title. 
24d18
< calculating a metric of document similarity. 
26,27d19
< (LD) algorithm, reducing them in size by a large factor, and proportionately extending 
< the range of input sizes over which LD is practical for a given set of time/space 
29d20
< Though less precise than LD alone, the technique estimates the Levenshtein
31d21
< practicably 
34d23
< Web pages, articles and books. 
37,38d25
< 
< %
42d28
< \section{Introduction}
44d29
< For strings $S_{1}$ and $S_{2}$, $LD(S_{1},S_{2})$ is the number of edits, i.e.,
46d30
< convert $S_{1}$ into $S_{2}$. 
50d33
< lengths of the two inputs ({\em O(m*n)}), and is thus useful only for strings of
55,56d37
< while strings of $100$ times that length, $4$KB, were processed at only $3.7$ pairs
< per second, $1/10000$th as fast.
64d44
< However, for many purposes, we don't really care about an exact metric;
67d46
< execute the algorithm in time proportional to the compressed length.
69,70d47
< distance measure, our choice of compression algorithms is unconstraind by the 
< requirement that we be able to re-inflate the output. 
75d51
< For example, a compression rate of $100$ to $1$ results in a speedup factor of
82d57
<   	\item {Choose $c$, a compression factor, say, $c=100$.}
85d59
< 			parameterized by $c$ and $n$ as described below.}
90c64,118
< Michael Mitzenmacher, Internet Mathematics, Vol. 1, No. 4: 485-509
---
> 
> \subsection{Compression}
> As mentioned above, the only differs from LD only in pre-compressing the inputs
> and scaling the result by the compression factor.
> 
> ``signature'' string that has the following properties: 
> \begin{enumerate}
> 	\item {
> 		The signature length is roughly proportional to the length of the input
> 		divided by a constant ($c$).
> 	}
> 	\item{
> 		whether it is compressed by itself or compressed embedded in 
> 		a larger string.
> 	}
> \end{enumerate}
> 
> and require only that the signature computed for a substring, when it is embedded in
> a larger string, should only differ in at most a few leading and trailing bits,
> 
> change any bit of the output that is not very close to that postion. 
> 
> \subsubsection{Compression Steps}
> A string is converted into a compressed signatures by the following algorithm.
> The parameters are defined above.
> \begin{enumerate}
> 	\item {Proceed from left to right on character at a time. At each position:
> 		\begin{enumerate}
> 			(Each neighborhood will overlap the previous by one $n-1$ characters.)}
> 			\item {If $i$ is equal to 1, emit 1.}
> 			\item {Otherwise, emit nothing.}
> 		\end{enumerate}
> 	}
> \end{enumerate}
> simple as maintaining a rolling sum of the numeric values of the $n$
> Thus, the computational cost of the hashing can be as low as one addition,
> one subtraction, and one mod operation per character to be compressed.
> 
> The resulting string has the properties mentioned above, and is highly
> randomized if the input text is reasonably diverse.
> As described above, a compression factor of $c$ results in a signature about
> $1/c$ as large as the original string.
> Note, however, that there is only one bit of information in each character.
> stored as bits, reducing storage requirements for pre-computed signatures 
> represents strings. 
> 
> As they are already highly randomized, it is impossible to compress them
> further.
> 
> 
> \section{What Is It Good For?}
> 
> 
> 
> \begin{thebibliography}{Purdom-Brown 85}
